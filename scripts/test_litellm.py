#!/usr/bin/env python3
"""
Script de teste para verificar funcionamento do LiteLLM Gateway
"""

import asyncio
import httpx
import json
import os
from datetime import datetime

# Configura√ß√µes
LITELLM_URL = os.getenv("LITELLM_URL", "http://localhost:4000")
LITELLM_API_KEY = os.getenv("LITELLM_MASTER_KEY", "sk-master-dev")

async def test_health():
    """Testa endpoint de health"""
    print("üîç Testando health check...")
    
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(f"{LITELLM_URL}/health")
            if response.status_code == 200:
                print("‚úÖ Health check OK")
                print(f"   Response: {response.json()}")
            else:
                print(f"‚ùå Health check falhou: {response.status_code}")
        except Exception as e:
            print(f"‚ùå Erro ao conectar: {e}")
            return False
    
    return True

async def test_list_models():
    """Testa listagem de modelos"""
    print("\nüîç Listando modelos dispon√≠veis...")
    
    async with httpx.AsyncClient() as client:
        response = await client.get(
            f"{LITELLM_URL}/ai/models",
            headers={"Authorization": f"Bearer {LITELLM_API_KEY}"}
        )
        
        if response.status_code == 200:
            models = response.json()["data"]
            print(f"‚úÖ {len(models)} modelos dispon√≠veis:")
            for model in models:
                print(f"   - {model['id']} (Tier {model['tier']}): {model['description']}")
        else:
            print(f"‚ùå Erro ao listar modelos: {response.status_code}")

async def test_chat_completion(tier="tier1"):
    """Testa chat completion"""
    print(f"\nüîç Testando chat completion com {tier}...")
    
    messages = [
        {"role": "system", "content": "Voc√™ √© um assistente √∫til e conciso."},
        {"role": "user", "content": "Ol√°! Me diga em uma frase: qual √© a capital do Brasil?"}
    ]
    
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.post(
            f"{LITELLM_URL}/ai/chat/completions",
            headers={
                "Authorization": f"Bearer {LITELLM_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "messages": messages,
                "metadata": {
                    "organization_id": "test_org",
                    "user_id": "test_user",
                    "preferred_tier": tier
                },
                "temperature": 0.7,
                "max_tokens": 100
            }
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ Resposta recebida:")
            print(f"   Modelo usado: {data.get('model', 'unknown')}")
            print(f"   Resposta: {data['choices'][0]['message']['content']}")
            
            if "usage" in data:
                usage = data["usage"]
                print(f"   Tokens: {usage.get('total_tokens', 0)} total")
        else:
            print(f"‚ùå Erro na completion: {response.status_code}")
            print(f"   Response: {response.text}")

async def test_streaming():
    """Testa streaming de respostas"""
    print("\nüîç Testando streaming...")
    
    messages = [
        {"role": "user", "content": "Conte uma piada curta sobre programa√ß√£o"}
    ]
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(
            f"{LITELLM_URL}/ai/chat/completions",
            headers={
                "Authorization": f"Bearer {LITELLM_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "messages": messages,
                "stream": True,
                "metadata": {
                    "organization_id": "test_org"
                }
            }
        )
        
        if response.status_code == 200:
            print("‚úÖ Streaming iniciado:")
            print("   Resposta: ", end="", flush=True)
            
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data = line[6:]
                    if data == "[DONE]":
                        print("\n   ‚úÖ Streaming conclu√≠do")
                        break
                    
                    try:
                        chunk = json.loads(data)
                        if "choices" in chunk and chunk["choices"]:
                            content = chunk["choices"][0].get("delta", {}).get("content", "")
                            print(content, end="", flush=True)
                    except json.JSONDecodeError:
                        pass
        else:
            print(f"‚ùå Erro no streaming: {response.status_code}")

async def test_routing():
    """Testa roteamento inteligente"""
    print("\nüîç Testando roteamento inteligente...")
    
    test_cases = [
        {
            "name": "Sauda√ß√£o simples",
            "message": "Oi!",
            "expected_tier": "tier1"
        },
        {
            "name": "Suporte t√©cnico",
            "message": "Estou com erro 500 na API quando fa√ßo POST no endpoint /users",
            "expected_tier": "tier2"
        },
        {
            "name": "Query complexa",
            "message": "Preciso de ajuda para implementar um sistema de autentica√ß√£o OAuth2 com refresh tokens e suporte a m√∫ltiplos provedores",
            "expected_tier": "tier3"
        }
    ]
    
    for test in test_cases:
        print(f"\n   üìù Teste: {test['name']}")
        print(f"   Mensagem: {test['message']}")
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{LITELLM_URL}/ai/chat/completions",
                headers={
                    "Authorization": f"Bearer {LITELLM_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "messages": [
                        {"role": "user", "content": test['message']}
                    ],
                    "metadata": {
                        "organization_id": "test_org",
                        "test_routing": True
                    },
                    "max_tokens": 50
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                model = data.get("model", "unknown")
                tier = model.split("/")[0] if "/" in model else "unknown"
                
                print(f"   Modelo usado: {model}")
                print(f"   Tier detectado: {tier}")
                
                if tier == test["expected_tier"]:
                    print(f"   ‚úÖ Roteamento correto!")
                else:
                    print(f"   ‚ö†Ô∏è  Esperado {test['expected_tier']}, recebido {tier}")

async def test_usage_stats():
    """Testa estat√≠sticas de uso"""
    print("\nüîç Testando estat√≠sticas de uso...")
    
    async with httpx.AsyncClient() as client:
        response = await client.get(
            f"{LITELLM_URL}/ai/usage",
            headers={"Authorization": f"Bearer {LITELLM_API_KEY}"},
            params={
                "organization_id": "test_org",
                "start_date": "2024-01-01",
                "end_date": datetime.now().isoformat()
            }
        )
        
        if response.status_code == 200:
            usage = response.json()
            print("‚úÖ Estat√≠sticas obtidas:")
            print(f"   Total de requisi√ß√µes: {usage['summary']['total_requests']}")
            print(f"   Total de tokens: {usage['summary']['total_tokens']}")
            print(f"   Custo total: ${usage['summary']['total_cost']:.4f}")
        else:
            print(f"‚ùå Erro ao obter estat√≠sticas: {response.status_code}")

async def main():
    """Executa todos os testes"""
    print("üöÄ Iniciando testes do LiteLLM Gateway")
    print(f"   URL: {LITELLM_URL}")
    print(f"   API Key: {LITELLM_API_KEY[:10]}...")
    
    # Testa health primeiro
    if not await test_health():
        print("\n‚ùå LiteLLM n√£o est√° respondendo. Verifique se o servi√ßo est√° rodando.")
        return
    
    # Executa outros testes
    await test_list_models()
    await test_chat_completion("tier1")
    await test_chat_completion("tier2")
    await test_streaming()
    await test_routing()
    await test_usage_stats()
    
    print("\n‚úÖ Testes conclu√≠dos!")

if __name__ == "__main__":
    asyncio.run(main())
