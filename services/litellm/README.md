# LiteLLM Gateway Service

O LiteLLM Gateway √© o servi√ßo centralizado para gerenciamento de modelos de IA no projeto Agentes de Convers√£o.

## üéØ Funcionalidades

### Roteamento Inteligente por Tiers

O gateway classifica e roteia requisi√ß√µes para diferentes modelos baseado em:

1. **Complexidade da mensagem** - An√°lise autom√°tica do conte√∫do
2. **Inten√ß√£o detectada** - Classifica√ß√£o do tipo de intera√ß√£o
3. **Valor do cliente** - VIPs recebem modelos superiores
4. **Plano da organiza√ß√£o** - Limites por tier de assinatura

### Tiers de Modelos

- **Tier 1 (Fast)**: Llama 3.2 3B, Gemini Flash - Para sauda√ß√µes, FAQ, queries simples
- **Tier 2 (Balanced)**: Claude Haiku, GPT-3.5 - Para suporte b√°sico, consultas de produto
- **Tier 3 (Advanced)**: Claude Sonnet, GPT-4 - Para suporte t√©cnico, queries complexas
- **Tier 4 (Premium)**: Claude Opus, GPT-4 Turbo - Para negocia√ß√µes, leads high-value

### Features

- ‚úÖ Fallback autom√°tico entre modelos
- ‚úÖ Cache Redis para respostas frequentes
- ‚úÖ Streaming de respostas
- ‚úÖ M√©tricas Prometheus
- ‚úÖ Rate limiting por organiza√ß√£o
- ‚úÖ Cost tracking em tempo real
- ‚úÖ Retry logic com backoff exponencial

## üöÄ Uso

### Endpoint Principal

```bash
POST /ai/chat/completions
Authorization: Bearer {LITELLM_API_KEY}

{
  "messages": [
    {"role": "system", "content": "Voc√™ √© um assistente √∫til"},
    {"role": "user", "content": "Ol√°!"}
  ],
  "metadata": {
    "organization_id": "org_123",
    "user_id": "user_456",
    "conversation_id": "conv_789",
    "intent": "greeting"
  },
  "stream": false
}
```

### Streaming

```javascript
const response = await fetch('/ai/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer sk-master-key'
  },
  body: JSON.stringify({
    messages: [...],
    stream: true
  })
});

const reader = response.body.getReader();
// Process streaming chunks...
```

### Listar Modelos

```bash
GET /ai/models
Authorization: Bearer {LITELLM_API_KEY}
```

### Analytics de Uso

```bash
GET /ai/usage?start_date=2024-01-01&end_date=2024-01-31
Authorization: Bearer {LITELLM_API_KEY}
```

## üîß Configura√ß√£o

### Vari√°veis de Ambiente

```env
# Chaves dos Provedores
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=AIza...
TOGETHER_API_KEY=...
GROQ_API_KEY=gsk_...

# Redis
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=...

# Master Key
LITELLM_MASTER_KEY=sk-master-production
```

### Arquivo de Configura√ß√£o

O arquivo `config/litellm_config.yaml` define:

1. Lista de modelos dispon√≠veis
2. Configura√ß√µes de fallback
3. Limites de rate
4. Regras de roteamento
5. Callbacks de monitoramento

## üìä M√©tricas

O servi√ßo exp√µe m√©tricas Prometheus em `/metrics`:

- `llm_requests_total` - Total de requisi√ß√µes por modelo/tier/status
- `llm_tokens_total` - Tokens processados (prompt/completion)
- `llm_latency_seconds` - Lat√™ncia das requisi√ß√µes
- `llm_cost_total` - Custo acumulado por organiza√ß√£o
- `llm_active_requests` - Requisi√ß√µes em andamento

## üõ°Ô∏è Seguran√ßa

1. **Autentica√ß√£o**: Bearer token obrigat√≥rio
2. **Rate Limiting**: Configur√°vel por organiza√ß√£o/plano
3. **Input Validation**: Sanitiza√ß√£o de prompts
4. **Cost Control**: Limites de gastos por organiza√ß√£o
5. **Audit Trail**: Todas requisi√ß√µes s√£o logadas

## üîÑ Fallback Strategy

```yaml
fallbacks:
  tier4/claude-opus: ["tier4/gpt-4-turbo", "tier3/claude-sonnet"]
  tier3/claude-sonnet: ["tier3/gpt-4", "tier2/claude-haiku"]
  tier2/claude-haiku: ["tier2/gpt-3.5-turbo", "tier1/llama-3.2-3b"]
  tier1/llama-3.2-3b: ["tier1/gemini-flash"]
```

## üêõ Troubleshooting

### Erro 503 - Service Unavailable
- Verificar se o modelo est√° dispon√≠vel
- Checar limites de rate
- Verificar chaves de API

### Streaming n√£o funciona
- Confirmar que o cliente suporta SSE
- Verificar proxy/nginx buffering
- Testar com `stream: false` primeiro

### Alto custo inesperado
- Revisar logs de roteamento
- Verificar se VIPs est√£o configurados corretamente
- Analisar m√©tricas de uso por modelo

## üìö Exemplos

### Python

```python
import httpx
import json

async def chat_with_litellm():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:4000/ai/chat/completions",
            headers={
                "Authorization": "Bearer sk-master-key",
                "Content-Type": "application/json"
            },
            json={
                "messages": [
                    {"role": "user", "content": "Ol√°!"}
                ],
                "metadata": {
                    "organization_id": "org_123"
                }
            }
        )
        return response.json()
```

### Node.js

```javascript
const response = await fetch('http://localhost:4000/ai/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': 'Bearer sk-master-key',
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    messages: [
      { role: 'user', content: 'Ol√°!' }
    ],
    metadata: {
      organization_id: 'org_123'
    }
  })
});

const data = await response.json();
console.log(data.choices[0].message.content);
```
