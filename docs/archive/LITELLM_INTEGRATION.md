# ğŸ¤– IntegraÃ§Ã£o LiteLLM - Agentes de ConversÃ£o

Este documento descreve a implementaÃ§Ã£o completa do LiteLLM como gateway centralizado de LLMs no projeto.

## ğŸ“‹ VisÃ£o Geral

O LiteLLM atua como gateway unificado para todos os modelos de IA, oferecendo:

- **Roteamento Inteligente**: SeleÃ§Ã£o automÃ¡tica de modelos baseada em complexidade, contexto e valor do cliente
- **Fallback AutomÃ¡tico**: MigraÃ§Ã£o transparente entre modelos em caso de falha
- **Cache Inteligente**: Respostas frequentes armazenadas em Redis
- **Controle de Custos**: Tracking detalhado por organizaÃ§Ã£o e modelo
- **Streaming**: Suporte nativo para respostas em tempo real

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚â”€â”€â”€â”€â–¶â”‚   API Gateway    â”‚â”€â”€â”€â”€â–¶â”‚  Backend API    â”‚
â”‚   (Next.js)     â”‚     â”‚   (Gateway)      â”‚     â”‚  (FastAPI)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                           â”‚
                                                           â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                                             â”‚
                        â–¼                                             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  LiteLLM Gatewayâ”‚                          â”‚  Knowledge Base  â”‚
              â”‚   (Port 4000)   â”‚                          â”‚    (Qdrant)      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼             â–¼              â–¼                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Claude   â”‚  â”‚  OpenAI  â”‚  â”‚  Gemini  â”‚    â”‚  Llama   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ ConfiguraÃ§Ã£o

### 1. VariÃ¡veis de Ambiente

```env
# LiteLLM
LITELLM_MASTER_KEY=sk-master-production-key
LITELLM_DATABASE_URL=postgresql://...

# Provedores de IA
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=AIza...
TOGETHER_API_KEY=...
GROQ_API_KEY=gsk_...

# Redis
REDIS_URL=redis://localhost:6379
```

### 2. ConfiguraÃ§Ã£o de Modelos

O arquivo `config/litellm_config.yaml` define os tiers de modelos:

- **Tier 1**: Modelos rÃ¡pidos para queries simples (Llama 3.2 3B, Gemini Flash)
- **Tier 2**: Modelos balanceados (Claude Haiku, GPT-3.5)
- **Tier 3**: Modelos avanÃ§ados (Claude Sonnet, GPT-4)
- **Tier 4**: Modelos premium (Claude Opus, GPT-4 Turbo)

## ğŸ”§ Componentes Implementados

### 1. AI Service (`ai_service.py`)
- Interface unificada para chamadas ao LiteLLM
- Suporte a streaming e respostas normais
- AnÃ¡lise de conversas e geraÃ§Ã£o de insights

### 2. AI Router (`ai_router.py`)
- AnÃ¡lise de complexidade de mensagens
- DetecÃ§Ã£o automÃ¡tica de intent
- Roteamento baseado em regras e contexto
- ConsideraÃ§Ã£o de valor do cliente (VIP)

### 3. Knowledge Service (`knowledge_service.py`)
- IntegraÃ§Ã£o com Qdrant para busca vetorial
- GeraÃ§Ã£o de embeddings via LiteLLM
- Cache de resultados de busca

### 4. Cache Service (`cache_service.py`)
- Cache de respostas de IA
- Tracking de uso e mÃ©tricas
- Rate limiting por organizaÃ§Ã£o

### 5. Celery Workers
- **ai_tasks.py**: Processamento assÃ­ncrono de conversas
- **analytics_tasks.py**: Coleta de mÃ©tricas e anÃ¡lise com Jarvis
- **knowledge_tasks.py**: Processamento de documentos
- **notification_tasks.py**: Envio de alertas e relatÃ³rios

## ğŸ“Š MÃ©tricas e Monitoramento

### MÃ©tricas Prometheus Expostas

- `llm_requests_total`: Total de requisiÃ§Ãµes por modelo/tier
- `llm_tokens_total`: Tokens processados
- `llm_latency_seconds`: LatÃªncia das requisiÃ§Ãµes
- `llm_cost_total`: Custo acumulado
- `llm_active_requests`: RequisiÃ§Ãµes em andamento

### Analytics Dashboard

```python
# Exemplo de uso das mÃ©tricas
GET /ai/usage?start_date=2024-01-01&end_date=2024-01-31

{
  "organization_id": "org_123",
  "summary": {
    "total_cost": 125.43,
    "total_tokens": 1234567,
    "total_requests": 5432
  },
  "by_model": {
    "tier1/llama-3.2-3b": {
      "requests": 3000,
      "tokens": 500000,
      "cost": 5.00
    },
    "tier3/claude-sonnet": {
      "requests": 1000,
      "tokens": 400000,
      "cost": 80.00
    }
  }
}
```

## ğŸ”„ Fluxo de RequisiÃ§Ã£o

1. **Frontend** envia mensagem para API
2. **Backend** analisa contexto e histÃ³rico
3. **AI Router** determina complexidade e tier apropriado
4. **Knowledge Service** busca contexto relevante (se configurado)
5. **Cache Service** verifica se existe resposta em cache
6. **LiteLLM Gateway** roteia para modelo apropriado
7. **Fallback** automÃ¡tico se modelo falhar
8. **Response** retorna ao usuÃ¡rio (streaming ou normal)
9. **Analytics** registra mÃ©tricas em background

## ğŸ§ª Testando a IntegraÃ§Ã£o

### Script de Teste Completo

```bash
# Executar todos os testes
python scripts/test_litellm.py

# Testes incluem:
# - Health check
# - Listagem de modelos
# - Chat completion (todos os tiers)
# - Streaming
# - Roteamento inteligente
# - EstatÃ­sticas de uso
```

### Teste Manual via cURL

```bash
# Testar chat completion
curl -X POST http://localhost:4000/ai/chat/completions \
  -H "Authorization: Bearer sk-master-dev" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "OlÃ¡!"}
    ],
    "metadata": {
      "organization_id": "org_123"
    }
  }'
```

## ğŸ› Troubleshooting

### Problema: Modelos nÃ£o respondem
```bash
# Verificar status do LiteLLM
curl http://localhost:4000/health

# Verificar logs
docker-compose logs litellm
```

### Problema: Alto custo inesperado
```python
# Verificar roteamento
GET /api/conversations/analytics/models

# Ajustar regras em ai_router.py
```

### Problema: Cache nÃ£o funciona
```bash
# Verificar Redis
redis-cli ping

# Limpar cache
redis-cli FLUSHDB
```

## ğŸ“ˆ OtimizaÃ§Ãµes Futuras

1. **Model Fine-tuning**: Treinar modelos especÃ­ficos por vertical
2. **A/B Testing**: Testar diferentes modelos para mesmas queries
3. **Predictive Routing**: ML para prever melhor modelo
4. **Cost Optimization**: AnÃ¡lise automÃ¡tica de custo/benefÃ­cio
5. **Multi-region**: Deploy distribuÃ­do para menor latÃªncia

## ğŸ”’ SeguranÃ§a

- Todas as chaves de API em variÃ¡veis de ambiente
- Rate limiting por organizaÃ§Ã£o e plano
- SanitizaÃ§Ã£o de prompts antes de envio
- Audit trail completo de todas requisiÃ§Ãµes
- Criptografia de dados sensÃ­veis em cache

## ğŸ“š ReferÃªncias

- [LiteLLM Documentation](https://docs.litellm.ai/)
- [Anthropic Claude API](https://docs.anthropic.com/)
- [OpenAI API Reference](https://platform.openai.com/docs/)
- [Google Gemini API](https://ai.google.dev/)
